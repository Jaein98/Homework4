---
title: "Homework 4"
author: "PSTAT 131/231"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```


```{r}

library(tidyverse)
library(tidymodels)
library(ggplot2)
library(visdat)
library(corrplot)
library(discrim)
library(klaR)
library(yardstick)
library(rlang)

set.seed(589)
data = read.csv('data/titanic.csv')
data$survived = factor(data$survived, levels = c('Yes', 'No'))
data$pclass = factor(data$pclass)
data
```






### Question 1

Split the data, stratifying on the outcome variable, `survived.`  You should choose the proportions to split the data into. Verify that the training and testing data sets have the appropriate number of observations. 


```{r}

data_split <- initial_split(data, prop = 0.70,
                                strata = survived)
titanic_train <- training(data_split)
titanic_test <- testing(data_split)

print(c(dim(titanic_train), dim(titanic_test)))

```




### Question 2

Fold the **training** data. Use *k*-fold cross-validation, with $k = 10$.

```{r}

cv_folds <- vfold_cv(titanic_train, v = 10)

cv_folds
```


### Question 3

In your own words, explain what we are doing in Question 2. What is *k*-fold cross-validation? Why should we use it, rather than simply fitting and testing models on the entire training set? If we **did** use the entire training set, what resampling method would that be?


   
   
Cross validation is resampling procedure used to evaluate machine learning models on a limited data sample for finding the best paramater models and reducing prediction error. The process of cross-validation is we divide the train set into 10 folds, which is the size of folds are depend on the size of train set. An one set of 10 folds are used as test set and the rest of them as a train. And then make a model with 9 folds under the parameter value and predict the test folds. Repear this for 9 folds, and calculate the mean MSE. It is more effective rather than simply fitting and testing models on the entire training set because cross validation can reduce the prediction error by repeating that process. If we did use the entire training set, Bootstrap could be used because this method does not divide train set as cross-validation.





### Question 4

Set up workflows for 3 models:

1. A logistic regression with the `glm` engine;
2. A linear discriminant analysis with the `MASS` engine;
3. A quadratic discriminant analysis with the `MASS` engine.

How many models, total, across all folds, will you be fitting to the data? To answer, think about how many folds there are, and how many models you'll fit to each fold.

```{r}
#Recipe
titanic_recipe = recipe(survived ~ pclass + sex + age + sib_sp + parch + fare, 
                        data = titanic_train) %>%
        step_impute_linear(age) %>%
        step_dummy(all_nominal_predictors()) %>%
        step_interact(terms = ~ starts_with("sex"):fare + age:fare)


#Logistic
log_reg = logistic_reg() %>% 
        set_engine("glm") %>% 
        set_mode("classification")

log_wkflow = workflow() %>% 
        add_model(log_reg) %>% 
        add_recipe(titanic_recipe)

log_fit = fit(log_wkflow, titanic_train)


#LDA
lda_mod = discrim_linear() %>%
        set_engine("MASS") %>%
        set_mode("classification")

lda_wkflow = workflow() %>% 
        add_model(lda_mod) %>% 
        add_recipe(titanic_recipe)

lda_fit = fit(lda_wkflow, titanic_train)


#QDA
qda_mod = discrim_quad() %>% 
        set_mode("classification") %>% 
        set_engine("MASS")

qda_wkflow = workflow() %>% 
        add_model(qda_mod) %>% 
        add_recipe(titanic_recipe)

qda_fit = fit(qda_wkflow, titanic_train)




log_fit
lda_fit
qda_fit


```






### Question 5

Fit each of the models created in Question 4 to the folded data.

```{r}
log_res = log_wkflow %>% 
        fit_resamples(resamples = cv_folds, 
                      metrics = metric_set(recall, precision, 
                                           accuracy, sens, spec, roc_auc),
                      control = control_resamples(save_pred = TRUE)) 

lda_res = lda_wkflow %>%
        fit_resamples(resamples = cv_folds,
                      metrics = metric_set(recall, precision, 
                                           accuracy, sens, spec, roc_auc),
                      control = control_resamples(save_pred = TRUE))

qda_res = qda_wkflow %>%
        fit_resamples(resamples = cv_folds,
                      metrics = metric_set(recall, precision, 
                                           accuracy, sens, spec, roc_auc),
                      control = control_resamples(save_pred = TRUE))
```

30 models 

### Question 6

Use `collect_metrics()` to print the mean and standard errors of the performance metric *accuracy* across all folds for each of the four models.

Decide which of the 3 fitted models has performed the best. Explain why. *(Note: You should consider both the mean accuracy and its standard error.)*


```{r}
collect_metrics(log_res)
collect_metrics(lda_res)
collect_metrics(qda_res)


```
log_res is the best since it has the highest accuracy with std of 0.01.

### Question 7

```{r}
log_fit = fit(log_wkflow, titanic_train)
log_fit

```


### Question 8

Finally, with your fitted model, use `predict()`, `bind_cols()`, and `accuracy()` to assess your model’s performance on the testing data!

Compare your model’s testing accuracy to its average accuracy across folds. Describe what you see.



```{r}
predict(log_fit, new_data = titanic_test, type = "prob")

log_acc = augment(log_fit, new_data = titanic_test) %>%
  accuracy(truth = survived, estimate = .pred_class)

log_acc

```












